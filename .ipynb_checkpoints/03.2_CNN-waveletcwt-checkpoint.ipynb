{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedf9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92db4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device Configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff276239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f2a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load your data from CSV file\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.length = len(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load and preprocess the data at the given index\n",
    "        sample = self.data.iloc[index]\n",
    "        \n",
    "        # Extract features and labels\n",
    "        features = torch.tensor(np.reshape(sample.iloc[0:4095].values, (1, 4095)), dtype=torch.float32)  # Adjust based on your column names\n",
    "#         label = torch.tensor(sample[['y0', 'y1']].values, dtype=torch.float32)  # Assuming label1 and label2 are column names\n",
    "        label = torch.tensor(np.asarray([sample['y0'], sample['y1']*100]), dtype=torch.float32)\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ba606",
   "metadata": {},
   "source": [
    "#### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adc5504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('./data/merged_data.csv')\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "val_loader = dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6e2734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Features: torch.Size([8, 1, 4095]), Labels: torch.Size([8, 2])\n",
      "First few elements of features:\n",
      "tensor([[[ 0.0791,  0.0911,  0.0931,  ...,  0.0100, -0.0137,  0.0003]],\n",
      "\n",
      "        [[ 0.1250,  0.1020,  0.1060,  ...,  0.0003,  0.0036,  0.0056]],\n",
      "\n",
      "        [[ 0.1010,  0.1220,  0.1020,  ...,  0.0097,  0.0066,  0.0019]],\n",
      "\n",
      "        [[ 0.1180,  0.0902,  0.1080,  ...,  0.0075, -0.0140,  0.0007]],\n",
      "\n",
      "        [[ 0.0851,  0.0801,  0.0850,  ...,  0.0044, -0.0065, -0.0065]]])\n",
      "First few elements of labels:\n",
      "tensor([[ 921.0000,    4.9000],\n",
      "        [ 937.0000,    7.4000],\n",
      "        [ 904.0000,    5.8000],\n",
      "        [1017.0000,   11.7000],\n",
      "        [ 986.0000,    8.9000],\n",
      "        [ 915.0000,    6.1000],\n",
      "        [1030.0000,    9.5000],\n",
      "        [ 911.0000,    6.1000]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1} - Features: {features.shape}, Labels: {labels.shape}\")\n",
    "    print(\"First few elements of features:\")\n",
    "    print(features[:5])\n",
    "    print(\"First few elements of labels:\")\n",
    "    print(labels)\n",
    "\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d26779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # To determine the input size of fully connected layer\n",
    "# dataiter = iter(train_loader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# conv1 = nn.Conv1d(1, 32, 3)\n",
    "# pool1 = nn.MaxPool1d(5)\n",
    "# conv2 = nn.Conv1d(32, 64, kernel_size=10)\n",
    "# pool2 = nn.MaxPool1d(5)\n",
    "# conv3 = nn.Conv1d(64,128,kernel_size=16)\n",
    "# print(images.shape)\n",
    "# y = conv1(images)\n",
    "# print(y.shape)\n",
    "# y = pool1(y)\n",
    "# print(y.shape)\n",
    "# y = conv2(y)\n",
    "# print(y.shape)\n",
    "# y = pool2(y)\n",
    "# print(y.shape)\n",
    "# y = conv3(y)\n",
    "# print(y.shape)\n",
    "\n",
    "# x = F.relu(conv1(images))\n",
    "# x = pool1(x)\n",
    "\n",
    "# x = F.relu(conv2(x))\n",
    "# x = pool2(x)\n",
    "\n",
    "# x = F.relu(conv3(x))\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3b0c4",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad15b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 32, 3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(10)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(32,64,10)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(10)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(64,64,16)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*25, 512)  # Adjust input size based on your data\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)  # 2 output values for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with batch normalization and ReLU activation\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52d077f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionCNN(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=10, stride=10, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(10,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=10, stride=10, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(64, 64, kernel_size=(16,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1600, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = RegressionCNN().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c83b18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a7020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "#-----------------------------------------------------\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "#-----------------------------------------------------\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Scheduler\n",
    "#-----------------------------------------------------\n",
    "# scheduler = ExponentialLR(optimizer, gamma = 0.1)\n",
    "# scheduler = StepLR(optimizer, step_size = 4, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "283ef294",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Training Loss: 462344.6406, Validation Loss: 467384.8438\n",
      "Epoch 2/300, Training Loss: 459055.2839, Validation Loss: 465252.1875\n",
      "Epoch 3/300, Training Loss: 452945.1719, Validation Loss: 460133.5938\n",
      "Epoch 4/300, Training Loss: 442456.6068, Validation Loss: 447745.3125\n",
      "Epoch 5/300, Training Loss: 426257.1146, Validation Loss: 425073.8750\n",
      "Epoch 6/300, Training Loss: 402901.9557, Validation Loss: 391765.5938\n",
      "Epoch 7/300, Training Loss: 371711.8516, Validation Loss: 352327.4688\n",
      "Epoch 8/300, Training Loss: 332397.9375, Validation Loss: 306864.1250\n",
      "Epoch 9/300, Training Loss: 285654.6745, Validation Loss: 254775.0000\n",
      "Epoch 10/300, Training Loss: 233328.2995, Validation Loss: 198116.6562\n",
      "Epoch 11/300, Training Loss: 178608.5352, Validation Loss: 142033.1562\n",
      "Epoch 12/300, Training Loss: 125945.8268, Validation Loss: 93053.4375\n",
      "Epoch 13/300, Training Loss: 79146.5124, Validation Loss: 55581.3828\n",
      "Epoch 14/300, Training Loss: 43156.9772, Validation Loss: 24683.2656\n",
      "Epoch 15/300, Training Loss: 19378.3785, Validation Loss: 10294.0820\n",
      "Epoch 16/300, Training Loss: 6742.3736, Validation Loss: 3345.9832\n",
      "Epoch 17/300, Training Loss: 1693.9551, Validation Loss: 888.7639\n",
      "Epoch 18/300, Training Loss: 515.8221, Validation Loss: 239.6759\n",
      "Epoch 19/300, Training Loss: 253.1314, Validation Loss: 117.7012\n",
      "Epoch 20/300, Training Loss: 335.1102, Validation Loss: 102.0580\n",
      "Epoch 21/300, Training Loss: 329.3959, Validation Loss: 87.3353\n",
      "Epoch 22/300, Training Loss: 236.8025, Validation Loss: 73.5538\n",
      "Epoch 23/300, Training Loss: 279.4249, Validation Loss: 94.1087\n",
      "Epoch 24/300, Training Loss: 252.6447, Validation Loss: 87.6185\n",
      "Epoch 25/300, Training Loss: 229.2026, Validation Loss: 122.5005\n",
      "Epoch 26/300, Training Loss: 229.8703, Validation Loss: 114.0976\n",
      "Epoch 27/300, Training Loss: 183.5952, Validation Loss: 94.9346\n",
      "Epoch 28/300, Training Loss: 202.3312, Validation Loss: 91.6725\n",
      "Epoch 29/300, Training Loss: 267.5431, Validation Loss: 173.4772\n",
      "Epoch 30/300, Training Loss: 135.3737, Validation Loss: 161.1689\n",
      "Epoch 31/300, Training Loss: 269.2095, Validation Loss: 154.8146\n",
      "Epoch 32/300, Training Loss: 185.0259, Validation Loss: 214.8768\n",
      "Epoch 33/300, Training Loss: 207.0998, Validation Loss: 64.8977\n",
      "Epoch 34/300, Training Loss: 256.4496, Validation Loss: 109.6962\n",
      "Epoch 35/300, Training Loss: 171.7967, Validation Loss: 116.6183\n",
      "Epoch 36/300, Training Loss: 171.0437, Validation Loss: 69.0042\n",
      "Epoch 37/300, Training Loss: 148.6219, Validation Loss: 85.7205\n",
      "Epoch 38/300, Training Loss: 151.2935, Validation Loss: 145.0303\n",
      "Epoch 39/300, Training Loss: 190.4091, Validation Loss: 222.5377\n",
      "Epoch 40/300, Training Loss: 185.2403, Validation Loss: 115.2498\n",
      "Epoch 41/300, Training Loss: 164.5779, Validation Loss: 104.0203\n",
      "Epoch 42/300, Training Loss: 155.4849, Validation Loss: 116.3473\n",
      "Epoch 43/300, Training Loss: 122.9150, Validation Loss: 175.4430\n",
      "Epoch 44/300, Training Loss: 154.8770, Validation Loss: 236.7173\n",
      "Epoch 45/300, Training Loss: 196.2027, Validation Loss: 291.6845\n",
      "Epoch 46/300, Training Loss: 216.8261, Validation Loss: 250.8860\n",
      "Epoch 47/300, Training Loss: 141.3176, Validation Loss: 91.5435\n",
      "Epoch 48/300, Training Loss: 153.7920, Validation Loss: 184.6384\n",
      "Epoch 49/300, Training Loss: 102.0233, Validation Loss: 116.6718\n",
      "Epoch 50/300, Training Loss: 103.9911, Validation Loss: 118.4028\n",
      "Epoch 51/300, Training Loss: 105.2271, Validation Loss: 110.1449\n",
      "Epoch 52/300, Training Loss: 148.1733, Validation Loss: 66.5767\n",
      "Epoch 53/300, Training Loss: 94.1681, Validation Loss: 110.0931\n",
      "Epoch 54/300, Training Loss: 114.3757, Validation Loss: 212.6625\n",
      "Epoch 55/300, Training Loss: 128.1769, Validation Loss: 168.2413\n",
      "Epoch 56/300, Training Loss: 92.5073, Validation Loss: 137.5561\n",
      "Epoch 57/300, Training Loss: 104.4088, Validation Loss: 80.2142\n",
      "Epoch 58/300, Training Loss: 97.8583, Validation Loss: 146.7228\n",
      "Epoch 59/300, Training Loss: 95.9253, Validation Loss: 65.4968\n",
      "Epoch 60/300, Training Loss: 98.0891, Validation Loss: 254.8688\n",
      "Epoch 61/300, Training Loss: 94.2559, Validation Loss: 165.0366\n",
      "Epoch 62/300, Training Loss: 94.9830, Validation Loss: 168.3028\n",
      "Epoch 63/300, Training Loss: 80.5827, Validation Loss: 64.8668\n",
      "Epoch 64/300, Training Loss: 109.6718, Validation Loss: 159.4899\n",
      "Epoch 65/300, Training Loss: 110.0237, Validation Loss: 145.7932\n",
      "Epoch 66/300, Training Loss: 97.4533, Validation Loss: 68.2280\n",
      "Epoch 67/300, Training Loss: 103.7121, Validation Loss: 458.9412\n",
      "Epoch 68/300, Training Loss: 88.3847, Validation Loss: 77.4753\n",
      "Epoch 69/300, Training Loss: 80.2477, Validation Loss: 64.7661\n",
      "Epoch 70/300, Training Loss: 108.2358, Validation Loss: 198.1280\n",
      "Epoch 71/300, Training Loss: 79.9641, Validation Loss: 223.8894\n",
      "Epoch 72/300, Training Loss: 79.5661, Validation Loss: 81.5249\n",
      "Epoch 73/300, Training Loss: 66.2609, Validation Loss: 63.2359\n",
      "Epoch 74/300, Training Loss: 137.5369, Validation Loss: 158.9755\n",
      "Epoch 75/300, Training Loss: 94.8520, Validation Loss: 88.4921\n",
      "Epoch 76/300, Training Loss: 86.3544, Validation Loss: 136.0698\n",
      "Epoch 77/300, Training Loss: 61.1765, Validation Loss: 99.4515\n",
      "Epoch 78/300, Training Loss: 67.2453, Validation Loss: 109.3280\n",
      "Epoch 79/300, Training Loss: 86.8511, Validation Loss: 73.9916\n",
      "Epoch 80/300, Training Loss: 71.7849, Validation Loss: 123.2437\n",
      "Epoch 81/300, Training Loss: 74.4582, Validation Loss: 139.2326\n",
      "Epoch 82/300, Training Loss: 77.5283, Validation Loss: 56.6492\n",
      "Epoch 83/300, Training Loss: 73.4294, Validation Loss: 101.9296\n",
      "Epoch 84/300, Training Loss: 68.0744, Validation Loss: 59.6901\n",
      "Epoch 85/300, Training Loss: 67.3949, Validation Loss: 114.5670\n",
      "Epoch 86/300, Training Loss: 61.8689, Validation Loss: 60.7210\n",
      "Epoch 87/300, Training Loss: 77.7811, Validation Loss: 142.0754\n",
      "Epoch 88/300, Training Loss: 64.1999, Validation Loss: 85.2578\n",
      "Epoch 89/300, Training Loss: 66.7392, Validation Loss: 133.3156\n",
      "Epoch 90/300, Training Loss: 74.0750, Validation Loss: 171.7274\n",
      "Epoch 91/300, Training Loss: 81.3236, Validation Loss: 162.6294\n",
      "Epoch 92/300, Training Loss: 60.3803, Validation Loss: 52.1674\n",
      "Epoch 93/300, Training Loss: 75.4721, Validation Loss: 68.9792\n",
      "Epoch 94/300, Training Loss: 70.8753, Validation Loss: 96.8341\n",
      "Epoch 95/300, Training Loss: 78.0624, Validation Loss: 47.8123\n",
      "Epoch 96/300, Training Loss: 63.5946, Validation Loss: 50.6690\n",
      "Epoch 97/300, Training Loss: 61.6527, Validation Loss: 56.5222\n",
      "Epoch 98/300, Training Loss: 56.0471, Validation Loss: 120.3023\n",
      "Epoch 99/300, Training Loss: 63.6976, Validation Loss: 90.4466\n",
      "Epoch 100/300, Training Loss: 56.4815, Validation Loss: 58.4226\n",
      "Epoch 101/300, Training Loss: 54.8660, Validation Loss: 70.0070\n",
      "Epoch 102/300, Training Loss: 67.4648, Validation Loss: 134.8619\n",
      "Epoch 103/300, Training Loss: 56.8386, Validation Loss: 51.4823\n",
      "Epoch 104/300, Training Loss: 55.5574, Validation Loss: 132.6886\n",
      "Epoch 105/300, Training Loss: 56.8465, Validation Loss: 123.9257\n",
      "Epoch 106/300, Training Loss: 63.7914, Validation Loss: 51.8313\n",
      "Epoch 107/300, Training Loss: 53.7361, Validation Loss: 52.2290\n",
      "Epoch 108/300, Training Loss: 70.0931, Validation Loss: 60.6883\n",
      "Epoch 109/300, Training Loss: 62.0776, Validation Loss: 68.7986\n",
      "Epoch 110/300, Training Loss: 52.1345, Validation Loss: 83.1397\n",
      "Epoch 111/300, Training Loss: 56.6491, Validation Loss: 65.6221\n",
      "Epoch 112/300, Training Loss: 61.3209, Validation Loss: 101.7241\n",
      "Epoch 113/300, Training Loss: 55.2712, Validation Loss: 52.3065\n",
      "Epoch 114/300, Training Loss: 55.7119, Validation Loss: 110.6329\n",
      "Epoch 115/300, Training Loss: 57.3522, Validation Loss: 165.8428\n",
      "Epoch 116/300, Training Loss: 63.0773, Validation Loss: 125.1508\n",
      "Epoch 117/300, Training Loss: 52.3094, Validation Loss: 66.7803\n",
      "Epoch 118/300, Training Loss: 60.9114, Validation Loss: 59.7914\n",
      "Epoch 119/300, Training Loss: 52.1765, Validation Loss: 75.2223\n",
      "Epoch 120/300, Training Loss: 56.7175, Validation Loss: 101.5561\n",
      "Epoch 121/300, Training Loss: 58.3046, Validation Loss: 63.4274\n",
      "Epoch 122/300, Training Loss: 51.6469, Validation Loss: 75.7367\n",
      "Epoch 123/300, Training Loss: 57.1807, Validation Loss: 60.1183\n",
      "Epoch 124/300, Training Loss: 54.5884, Validation Loss: 52.8781\n",
      "Epoch 125/300, Training Loss: 58.4762, Validation Loss: 50.4209\n",
      "Epoch 126/300, Training Loss: 57.8315, Validation Loss: 192.6305\n",
      "Epoch 127/300, Training Loss: 60.3056, Validation Loss: 81.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/300, Training Loss: 56.4904, Validation Loss: 51.8913\n",
      "Epoch 129/300, Training Loss: 54.7625, Validation Loss: 72.3731\n",
      "Epoch 130/300, Training Loss: 56.4755, Validation Loss: 48.0354\n",
      "Epoch 131/300, Training Loss: 55.3919, Validation Loss: 55.1615\n",
      "Epoch 132/300, Training Loss: 63.7896, Validation Loss: 59.9313\n",
      "Epoch 133/300, Training Loss: 56.5485, Validation Loss: 62.6890\n",
      "Epoch 134/300, Training Loss: 53.5369, Validation Loss: 70.6868\n",
      "Epoch 135/300, Training Loss: 59.3724, Validation Loss: 46.9167\n",
      "Epoch 136/300, Training Loss: 55.3095, Validation Loss: 57.8351\n",
      "Epoch 137/300, Training Loss: 56.9401, Validation Loss: 61.7939\n",
      "Epoch 138/300, Training Loss: 62.0437, Validation Loss: 78.2798\n",
      "Epoch 139/300, Training Loss: 54.8568, Validation Loss: 91.0009\n",
      "Epoch 140/300, Training Loss: 54.9454, Validation Loss: 52.0946\n",
      "Epoch 141/300, Training Loss: 60.5851, Validation Loss: 58.5666\n",
      "Epoch 142/300, Training Loss: 58.4393, Validation Loss: 57.7878\n",
      "Epoch 143/300, Training Loss: 49.5229, Validation Loss: 50.0723\n",
      "Epoch 144/300, Training Loss: 52.5820, Validation Loss: 58.9842\n",
      "Epoch 145/300, Training Loss: 53.2934, Validation Loss: 150.0845\n",
      "Epoch 146/300, Training Loss: 52.3993, Validation Loss: 56.3402\n",
      "Epoch 147/300, Training Loss: 57.2057, Validation Loss: 96.0411\n",
      "Epoch 148/300, Training Loss: 56.2495, Validation Loss: 51.8750\n",
      "Epoch 149/300, Training Loss: 50.4851, Validation Loss: 67.8821\n",
      "Epoch 150/300, Training Loss: 56.0389, Validation Loss: 50.9966\n",
      "Epoch 151/300, Training Loss: 49.6628, Validation Loss: 47.5831\n",
      "Epoch 152/300, Training Loss: 65.9595, Validation Loss: 117.7430\n",
      "Epoch 153/300, Training Loss: 58.3824, Validation Loss: 180.9631\n",
      "Epoch 154/300, Training Loss: 57.2437, Validation Loss: 73.8430\n",
      "Epoch 155/300, Training Loss: 57.5233, Validation Loss: 245.4273\n",
      "Epoch 156/300, Training Loss: 64.4750, Validation Loss: 45.4949\n",
      "Epoch 157/300, Training Loss: 56.5864, Validation Loss: 180.1223\n",
      "Epoch 158/300, Training Loss: 58.7393, Validation Loss: 253.6084\n",
      "Epoch 159/300, Training Loss: 57.8592, Validation Loss: 75.8976\n",
      "Epoch 160/300, Training Loss: 65.4930, Validation Loss: 54.4152\n",
      "Epoch 161/300, Training Loss: 59.3096, Validation Loss: 76.2193\n",
      "Epoch 162/300, Training Loss: 63.5204, Validation Loss: 78.0453\n",
      "Epoch 163/300, Training Loss: 67.4081, Validation Loss: 146.9914\n",
      "Epoch 164/300, Training Loss: 59.0401, Validation Loss: 94.9009\n",
      "Epoch 165/300, Training Loss: 53.7762, Validation Loss: 46.3056\n",
      "Epoch 166/300, Training Loss: 54.8862, Validation Loss: 135.3787\n",
      "Epoch 167/300, Training Loss: 58.7242, Validation Loss: 282.9194\n",
      "Epoch 168/300, Training Loss: 68.8427, Validation Loss: 142.1285\n",
      "Epoch 169/300, Training Loss: 67.8009, Validation Loss: 75.1776\n",
      "Epoch 170/300, Training Loss: 54.2823, Validation Loss: 49.2402\n",
      "Epoch 171/300, Training Loss: 48.3964, Validation Loss: 53.9871\n",
      "Epoch 172/300, Training Loss: 55.1419, Validation Loss: 151.3783\n",
      "Epoch 173/300, Training Loss: 50.1210, Validation Loss: 159.5636\n",
      "Epoch 174/300, Training Loss: 52.1938, Validation Loss: 106.5851\n",
      "Epoch 175/300, Training Loss: 48.1821, Validation Loss: 73.6618\n",
      "Epoch 176/300, Training Loss: 49.6445, Validation Loss: 96.0144\n",
      "Epoch 177/300, Training Loss: 52.5504, Validation Loss: 77.7241\n",
      "Epoch 178/300, Training Loss: 53.3775, Validation Loss: 69.6524\n",
      "Epoch 179/300, Training Loss: 52.7947, Validation Loss: 52.6201\n",
      "Epoch 180/300, Training Loss: 54.8614, Validation Loss: 59.6841\n",
      "Epoch 181/300, Training Loss: 58.9989, Validation Loss: 61.4358\n",
      "Epoch 182/300, Training Loss: 56.2050, Validation Loss: 183.1087\n",
      "Epoch 183/300, Training Loss: 53.4819, Validation Loss: 84.0303\n",
      "Epoch 184/300, Training Loss: 52.2400, Validation Loss: 69.2772\n",
      "Epoch 185/300, Training Loss: 52.8005, Validation Loss: 51.6624\n",
      "Epoch 186/300, Training Loss: 53.3738, Validation Loss: 62.0193\n",
      "Epoch 187/300, Training Loss: 55.0294, Validation Loss: 50.0392\n",
      "Epoch 188/300, Training Loss: 52.5337, Validation Loss: 50.3687\n",
      "Epoch 189/300, Training Loss: 53.1296, Validation Loss: 70.8572\n",
      "Epoch 190/300, Training Loss: 53.3885, Validation Loss: 44.6198\n",
      "Epoch 191/300, Training Loss: 52.1816, Validation Loss: 53.8531\n",
      "Epoch 192/300, Training Loss: 49.9333, Validation Loss: 60.0916\n",
      "Epoch 193/300, Training Loss: 52.9174, Validation Loss: 87.7788\n",
      "Epoch 194/300, Training Loss: 56.5859, Validation Loss: 46.9892\n",
      "Epoch 195/300, Training Loss: 53.8377, Validation Loss: 119.2280\n",
      "Epoch 196/300, Training Loss: 50.6028, Validation Loss: 63.3927\n",
      "Epoch 197/300, Training Loss: 51.2373, Validation Loss: 98.2207\n",
      "Epoch 198/300, Training Loss: 54.1453, Validation Loss: 103.8442\n",
      "Epoch 199/300, Training Loss: 54.1956, Validation Loss: 48.4734\n",
      "Epoch 200/300, Training Loss: 53.9890, Validation Loss: 50.7699\n",
      "Epoch 201/300, Training Loss: 51.2327, Validation Loss: 45.2763\n",
      "Epoch 202/300, Training Loss: 50.0172, Validation Loss: 73.4428\n",
      "Epoch 203/300, Training Loss: 49.1677, Validation Loss: 48.4007\n",
      "Epoch 204/300, Training Loss: 52.6985, Validation Loss: 53.5057\n",
      "Epoch 205/300, Training Loss: 49.1492, Validation Loss: 50.7583\n",
      "Epoch 206/300, Training Loss: 52.2987, Validation Loss: 52.6983\n",
      "Epoch 207/300, Training Loss: 53.0448, Validation Loss: 50.3083\n",
      "Epoch 208/300, Training Loss: 57.1953, Validation Loss: 43.7410\n",
      "Epoch 209/300, Training Loss: 50.6424, Validation Loss: 75.0435\n",
      "Epoch 210/300, Training Loss: 47.6117, Validation Loss: 46.6714\n",
      "Epoch 211/300, Training Loss: 49.6125, Validation Loss: 47.2769\n",
      "Epoch 212/300, Training Loss: 47.8344, Validation Loss: 61.8681\n",
      "Epoch 213/300, Training Loss: 47.1289, Validation Loss: 55.3229\n",
      "Epoch 214/300, Training Loss: 50.3882, Validation Loss: 54.3849\n",
      "Epoch 215/300, Training Loss: 50.9411, Validation Loss: 53.0052\n",
      "Epoch 216/300, Training Loss: 53.0931, Validation Loss: 45.1369\n",
      "Epoch 217/300, Training Loss: 49.5503, Validation Loss: 48.5890\n",
      "Epoch 218/300, Training Loss: 48.9806, Validation Loss: 67.6352\n",
      "Epoch 219/300, Training Loss: 48.2260, Validation Loss: 60.3492\n",
      "Epoch 220/300, Training Loss: 52.0073, Validation Loss: 48.6376\n",
      "Epoch 221/300, Training Loss: 50.3105, Validation Loss: 80.0839\n",
      "Epoch 222/300, Training Loss: 49.5642, Validation Loss: 46.4948\n",
      "Epoch 223/300, Training Loss: 53.7329, Validation Loss: 52.0738\n",
      "Epoch 224/300, Training Loss: 57.5672, Validation Loss: 55.6873\n",
      "Epoch 225/300, Training Loss: 56.9793, Validation Loss: 254.6092\n",
      "Epoch 226/300, Training Loss: 60.5831, Validation Loss: 208.2904\n",
      "Epoch 227/300, Training Loss: 53.4898, Validation Loss: 132.1000\n",
      "Epoch 228/300, Training Loss: 56.3888, Validation Loss: 50.2430\n",
      "Epoch 229/300, Training Loss: 53.3461, Validation Loss: 50.0774\n",
      "Epoch 230/300, Training Loss: 53.6600, Validation Loss: 60.1186\n",
      "Epoch 231/300, Training Loss: 58.9919, Validation Loss: 64.0024\n",
      "Epoch 232/300, Training Loss: 54.9094, Validation Loss: 76.2025\n",
      "Epoch 233/300, Training Loss: 55.7897, Validation Loss: 47.5804\n",
      "Epoch 234/300, Training Loss: 57.4160, Validation Loss: 75.2952\n",
      "Epoch 235/300, Training Loss: 54.5473, Validation Loss: 53.7498\n",
      "Epoch 236/300, Training Loss: 51.8211, Validation Loss: 40.3407\n",
      "Epoch 237/300, Training Loss: 47.9930, Validation Loss: 47.5758\n",
      "Epoch 238/300, Training Loss: 53.4651, Validation Loss: 129.1220\n",
      "Epoch 239/300, Training Loss: 57.9724, Validation Loss: 49.4862\n",
      "Epoch 240/300, Training Loss: 47.8922, Validation Loss: 47.7175\n",
      "Epoch 241/300, Training Loss: 49.0308, Validation Loss: 45.0647\n",
      "Epoch 242/300, Training Loss: 53.9629, Validation Loss: 42.5659\n",
      "Epoch 243/300, Training Loss: 53.9259, Validation Loss: 41.6921\n",
      "Epoch 244/300, Training Loss: 48.7649, Validation Loss: 100.6626\n",
      "Epoch 245/300, Training Loss: 48.6911, Validation Loss: 60.3025\n",
      "Epoch 246/300, Training Loss: 54.2994, Validation Loss: 54.5912\n",
      "Epoch 247/300, Training Loss: 51.9936, Validation Loss: 44.4963\n",
      "Epoch 248/300, Training Loss: 58.2578, Validation Loss: 39.4823\n",
      "Epoch 249/300, Training Loss: 52.7035, Validation Loss: 45.3340\n",
      "Epoch 250/300, Training Loss: 46.8570, Validation Loss: 67.8162\n",
      "Epoch 251/300, Training Loss: 49.4957, Validation Loss: 54.6402\n",
      "Epoch 252/300, Training Loss: 47.9652, Validation Loss: 45.8375\n",
      "Epoch 253/300, Training Loss: 47.7344, Validation Loss: 55.6286\n",
      "Epoch 254/300, Training Loss: 50.6768, Validation Loss: 207.0420\n",
      "Epoch 255/300, Training Loss: 50.1243, Validation Loss: 63.9727\n",
      "Epoch 256/300, Training Loss: 50.5036, Validation Loss: 81.6518\n",
      "Epoch 257/300, Training Loss: 50.2956, Validation Loss: 50.6864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258/300, Training Loss: 47.2465, Validation Loss: 57.5419\n",
      "Epoch 259/300, Training Loss: 49.9731, Validation Loss: 61.7089\n",
      "Epoch 260/300, Training Loss: 50.2761, Validation Loss: 59.3886\n",
      "Epoch 261/300, Training Loss: 54.1424, Validation Loss: 50.2549\n",
      "Epoch 262/300, Training Loss: 50.4348, Validation Loss: 55.9168\n",
      "Epoch 263/300, Training Loss: 50.4174, Validation Loss: 47.5875\n",
      "Epoch 264/300, Training Loss: 52.4580, Validation Loss: 65.3180\n",
      "Epoch 265/300, Training Loss: 49.7555, Validation Loss: 75.3482\n",
      "Epoch 266/300, Training Loss: 53.4009, Validation Loss: 94.1595\n",
      "Epoch 267/300, Training Loss: 52.0291, Validation Loss: 116.2584\n",
      "Epoch 268/300, Training Loss: 52.7152, Validation Loss: 57.2184\n",
      "Epoch 269/300, Training Loss: 49.2928, Validation Loss: 56.1699\n",
      "Epoch 270/300, Training Loss: 49.0490, Validation Loss: 46.6270\n",
      "Epoch 271/300, Training Loss: 50.5349, Validation Loss: 49.5037\n",
      "Epoch 272/300, Training Loss: 51.8027, Validation Loss: 137.0465\n",
      "Epoch 273/300, Training Loss: 48.8472, Validation Loss: 47.7399\n",
      "Epoch 274/300, Training Loss: 48.6210, Validation Loss: 62.8863\n",
      "Epoch 275/300, Training Loss: 47.8101, Validation Loss: 53.7061\n",
      "Epoch 276/300, Training Loss: 52.6328, Validation Loss: 53.9617\n",
      "Epoch 277/300, Training Loss: 50.8025, Validation Loss: 48.2770\n",
      "Epoch 278/300, Training Loss: 53.3695, Validation Loss: 48.0035\n",
      "Epoch 279/300, Training Loss: 49.4901, Validation Loss: 76.3009\n",
      "Epoch 280/300, Training Loss: 50.2600, Validation Loss: 46.5364\n",
      "Epoch 281/300, Training Loss: 48.9553, Validation Loss: 71.8214\n",
      "Epoch 282/300, Training Loss: 49.7639, Validation Loss: 49.4675\n",
      "Epoch 283/300, Training Loss: 53.8534, Validation Loss: 197.0798\n",
      "Epoch 284/300, Training Loss: 47.4495, Validation Loss: 94.0057\n",
      "Epoch 285/300, Training Loss: 49.1809, Validation Loss: 45.6031\n",
      "Epoch 286/300, Training Loss: 50.6397, Validation Loss: 54.8449\n",
      "Epoch 287/300, Training Loss: 50.3060, Validation Loss: 48.4846\n",
      "Epoch 288/300, Training Loss: 49.6989, Validation Loss: 53.7163\n",
      "Epoch 289/300, Training Loss: 48.3449, Validation Loss: 44.6882\n",
      "Epoch 290/300, Training Loss: 47.5739, Validation Loss: 84.4332\n",
      "Epoch 291/300, Training Loss: 49.9421, Validation Loss: 74.7979\n",
      "Epoch 292/300, Training Loss: 52.0421, Validation Loss: 50.0691\n",
      "Epoch 293/300, Training Loss: 56.1991, Validation Loss: 170.0818\n",
      "Epoch 294/300, Training Loss: 53.1848, Validation Loss: 83.4852\n",
      "Epoch 295/300, Training Loss: 50.6499, Validation Loss: 58.3148\n",
      "Epoch 296/300, Training Loss: 53.9959, Validation Loss: 58.2128\n",
      "Epoch 297/300, Training Loss: 50.4277, Validation Loss: 107.0764\n",
      "Epoch 298/300, Training Loss: 48.4491, Validation Loss: 110.5538\n",
      "Epoch 299/300, Training Loss: 48.9391, Validation Loss: 120.0693\n",
      "Epoch 300/300, Training Loss: 51.2632, Validation Loss: 73.7765\n",
      "tensor([[1003.4545,    9.5936],\n",
      "        [1009.6083,   10.2357],\n",
      "        [ 982.1768,   10.1060],\n",
      "        [ 886.5890,    5.3366],\n",
      "        [ 956.0476,    7.0474],\n",
      "        [1037.0372,   12.1296],\n",
      "        [1036.3939,   12.0713],\n",
      "        [ 935.7682,    6.5277],\n",
      "        [1014.3517,   11.0821],\n",
      "        [ 880.9882,    5.2563],\n",
      "        [ 933.1229,    6.4098],\n",
      "        [ 963.8007,    7.9860],\n",
      "        [1003.0129,    9.7506],\n",
      "        [ 890.4131,    5.2711],\n",
      "        [ 933.1326,    6.6493],\n",
      "        [ 901.8778,    5.6101],\n",
      "        [1022.0458,   10.2457],\n",
      "        [ 933.2458,    6.3035],\n",
      "        [ 932.7939,    6.3843],\n",
      "        [ 950.5993,    6.7794],\n",
      "        [1014.0139,   10.9064],\n",
      "        [ 898.9344,    5.3754],\n",
      "        [ 954.7299,    6.9875],\n",
      "        [ 943.3829,    6.8271]], device='cuda:0')\n",
      "tensor([[1008.0000,   11.0000],\n",
      "        [1021.0000,   11.6000],\n",
      "        [1005.0000,    9.4000],\n",
      "        [ 900.0000,    7.0000],\n",
      "        [ 944.0000,    5.0000],\n",
      "        [1048.0000,    9.8000],\n",
      "        [1057.0000,    4.6000],\n",
      "        [ 940.0000,    5.8000],\n",
      "        [1030.0000,   12.6000],\n",
      "        [ 879.0000,    6.8000],\n",
      "        [ 935.0000,    5.1000],\n",
      "        [ 991.0000,    8.2000],\n",
      "        [1007.0000,   10.2000],\n",
      "        [ 898.0000,    6.4000],\n",
      "        [ 940.0000,    7.9000],\n",
      "        [ 907.0000,    4.9000],\n",
      "        [1017.0000,   11.3000],\n",
      "        [ 942.0000,    8.1000],\n",
      "        [ 932.0000,    5.3000],\n",
      "        [ 943.0000,    4.9000],\n",
      "        [1035.0000,   11.9000],\n",
      "        [ 897.0000,    5.8000],\n",
      "        [ 953.0000,    8.9000],\n",
      "        [ 955.0000,    4.9000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Initialize variable to track loss and accuracies for each epochs\n",
    "y_loss = {} \n",
    "y_loss['train'] = []\n",
    "y_loss['val'] = []\n",
    "y_err = {}\n",
    "y_err['train'] = []\n",
    "y_err['val'] = []\n",
    "x_epoch = []\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "dataset_sizes = {'train': len(train_loader), 'val': len(val_loader)}\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs.to(device))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets.to(device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            val_outputs = model(val_inputs.to(device))\n",
    "            val_loss += criterion(val_outputs, val_targets.to(device)).item()\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss:.4f}, Validation Loss: {average_val_loss:.4f}')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_inputs, val_targets in val_loader:\n",
    "        val_outputs = model(val_inputs.to(device))\n",
    "        print(val_outputs)\n",
    "        print(val_targets.to(device))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625daf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbb28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90c605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed327b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0400d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cdd92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233ca52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e2a91316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # start\n",
    "# start = time.time()\n",
    "# for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "#     for phase in ['train', 'val']:        \n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0.0\n",
    "#         count = 0\n",
    "#         total = 0\n",
    "#         if(phase == 'train'):\n",
    "#             for images, labels in train_loader:\n",
    "#                 images = images.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "        \n",
    "#                 # Forward pass\n",
    "#                 outputs = model(images)\n",
    "#                 _, preds = torch.max(outputs.data, 1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "#                 # Backward and optimize\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 # the class with the highest energy is what we choose as prediction        \n",
    "#                 running_loss += loss.item()              \n",
    "#                 total += labels.size(0)\n",
    "#                 running_corrects += (preds == labels).sum().item()\n",
    "        \n",
    "#             epoch_loss = running_loss / total\n",
    "#             epoch_acc = running_corrects / total\n",
    "# #             if (i+1) % 20 == 0:\n",
    "# #             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "#             y_loss[phase].append(epoch_loss)\n",
    "#             y_err[phase].append(1.0 - epoch_acc)\n",
    "# #             scheduler.step()\n",
    "\n",
    "            \n",
    "#         elif(phase == 'val'):\n",
    "#             for i, (images, labels) in enumerate(val_loader):\n",
    "#                 images = images.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(images)\n",
    "#                 _, preds = torch.max(outputs.data, 1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 # the class with the highest energy is what we choose as prediction\n",
    "#                 running_loss += loss.item()              \n",
    "#                 total += labels.size(0)\n",
    "#                 running_corrects += (preds == labels).sum().item()               \n",
    "                \n",
    "#             epoch_loss = running_loss / total\n",
    "#             epoch_acc = running_corrects / total\n",
    "# #             if (i+1) % 200 == 0:\n",
    "# #             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "#             y_loss[phase].append(epoch_loss)\n",
    "#             y_err[phase].append(1.0 - epoch_acc)\n",
    "\n",
    "#             x_epoch.append(epoch)  \n",
    "            \n",
    "# print(f'training time: {(time.time()-start)} sec')\n",
    "# # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c31a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022008c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed5b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9caef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf5c0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
