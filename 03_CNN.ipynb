{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedf9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92db4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device Configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff276239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b4f2a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load your data from CSV file\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.length = len(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load and preprocess the data at the given index\n",
    "        sample = self.data.iloc[index]\n",
    "        \n",
    "        # Extract features and labels\n",
    "        features = torch.tensor(np.reshape(sample.iloc[0:4095].values, (1, 4095)), dtype=torch.float32)  # Adjust based on your column names\n",
    "#         label = torch.tensor(sample[['y0', 'y1']].values, dtype=torch.float32)  # Assuming label1 and label2 are column names\n",
    "        label = torch.tensor(np.asarray([sample['y0'], sample['y1']*100]), dtype=torch.float32)\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ba606",
   "metadata": {},
   "source": [
    "#### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "adc5504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('./data/merged_data.csv')\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "val_loader = dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee6e2734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Features: torch.Size([8, 1, 4095]), Labels: torch.Size([8, 2])\n",
      "First few elements of features:\n",
      "tensor([[[ 7.0300e-02,  7.5600e-02,  8.0000e-02,  ..., -1.2500e-02,\n",
      "          -6.9700e-03,  1.6000e-05]],\n",
      "\n",
      "        [[ 9.8200e-02,  1.0600e-01,  8.0400e-02,  ..., -3.2500e-03,\n",
      "          -5.4100e-03, -1.9900e-03]],\n",
      "\n",
      "        [[-9.0300e-03,  7.3200e-03,  1.6700e-03,  ...,  5.5900e-03,\n",
      "           3.7300e-03, -1.1000e-03]],\n",
      "\n",
      "        [[ 8.3500e-02,  8.2200e-02,  9.0000e-02,  ...,  2.1700e-03,\n",
      "           1.0600e-02, -1.6900e-03]],\n",
      "\n",
      "        [[ 9.2400e-02,  9.4400e-02,  9.7500e-02,  ..., -5.5600e-03,\n",
      "           2.2400e-03,  9.7900e-04]]])\n",
      "First few elements of labels:\n",
      "tensor([[1032.0000,   12.1000],\n",
      "        [ 982.0000,    4.4000],\n",
      "        [ 869.0000,    6.2000],\n",
      "        [ 953.0000,    7.5000],\n",
      "        [ 953.0000,    5.2000],\n",
      "        [1010.0000,   10.2000],\n",
      "        [ 942.0000,    5.5000],\n",
      "        [1066.0000,    3.9000]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1} - Features: {features.shape}, Labels: {labels.shape}\")\n",
    "    print(\"First few elements of features:\")\n",
    "    print(features[:5])\n",
    "    print(\"First few elements of labels:\")\n",
    "    print(labels)\n",
    "\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d1d26779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # To determine the input size of fully connected layer\n",
    "# dataiter = iter(train_loader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# conv1 = nn.Conv1d(1, 32, 3)\n",
    "# pool1 = nn.MaxPool1d(5)\n",
    "# conv2 = nn.Conv1d(32, 64, kernel_size=10)\n",
    "# pool2 = nn.MaxPool1d(5)\n",
    "# conv3 = nn.Conv1d(64,128,kernel_size=16)\n",
    "# print(images.shape)\n",
    "# y = conv1(images)\n",
    "# print(y.shape)\n",
    "# y = pool1(y)\n",
    "# print(y.shape)\n",
    "# y = conv2(y)\n",
    "# print(y.shape)\n",
    "# y = pool2(y)\n",
    "# print(y.shape)\n",
    "# y = conv3(y)\n",
    "# print(y.shape)\n",
    "\n",
    "# x = F.relu(conv1(images))\n",
    "# x = pool1(x)\n",
    "\n",
    "# x = F.relu(conv2(x))\n",
    "# x = pool2(x)\n",
    "\n",
    "# x = F.relu(conv3(x))\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3b0c4",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ad15b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv1d(32,64,10)\n",
    "        self.conv3 = nn.Conv1d(64,64,16)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool1d(10)\n",
    "        self.pool2 = nn.MaxPool1d(10)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64*25, 512)  # Adjust input size based on your data\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)  # 2 output values for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with batch normalization and ReLU activation\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "52d077f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionCNN(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(10,), stride=(1,))\n",
      "  (conv3): Conv1d(64, 64, kernel_size=(16,), stride=(1,))\n",
      "  (pool1): MaxPool1d(kernel_size=10, stride=10, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool2): MaxPool1d(kernel_size=10, stride=10, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1600, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = RegressionCNN().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c83b18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "23a7020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "#-----------------------------------------------------\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "#-----------------------------------------------------\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Scheduler\n",
    "#-----------------------------------------------------\n",
    "# scheduler = ExponentialLR(optimizer, gamma = 0.1)\n",
    "# scheduler = StepLR(optimizer, step_size = 4, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "283ef294",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 140.8382, Validation Loss: 132.5714\n",
      "Epoch 2/100, Training Loss: 96.7665, Validation Loss: 114.7989\n",
      "Epoch 3/100, Training Loss: 95.3681, Validation Loss: 105.0926\n",
      "Epoch 4/100, Training Loss: 95.4476, Validation Loss: 102.5401\n",
      "Epoch 5/100, Training Loss: 96.4200, Validation Loss: 115.2832\n",
      "Epoch 6/100, Training Loss: 86.0831, Validation Loss: 95.3997\n",
      "Epoch 7/100, Training Loss: 88.5067, Validation Loss: 92.7665\n",
      "Epoch 8/100, Training Loss: 87.4978, Validation Loss: 112.1006\n",
      "Epoch 9/100, Training Loss: 96.1083, Validation Loss: 91.9592\n",
      "Epoch 10/100, Training Loss: 84.2226, Validation Loss: 92.8733\n",
      "Epoch 11/100, Training Loss: 96.5164, Validation Loss: 99.2793\n",
      "Epoch 12/100, Training Loss: 103.8447, Validation Loss: 92.8744\n",
      "Epoch 13/100, Training Loss: 90.9898, Validation Loss: 103.6175\n",
      "Epoch 14/100, Training Loss: 85.7157, Validation Loss: 91.1573\n",
      "Epoch 15/100, Training Loss: 81.6151, Validation Loss: 98.7548\n",
      "Epoch 16/100, Training Loss: 89.1510, Validation Loss: 169.2558\n",
      "Epoch 17/100, Training Loss: 122.0991, Validation Loss: 137.8740\n",
      "Epoch 18/100, Training Loss: 91.4196, Validation Loss: 104.2530\n",
      "Epoch 19/100, Training Loss: 82.1610, Validation Loss: 115.5650\n",
      "Epoch 20/100, Training Loss: 108.2018, Validation Loss: 130.5303\n",
      "Epoch 21/100, Training Loss: 95.6939, Validation Loss: 93.7927\n",
      "Epoch 22/100, Training Loss: 79.0643, Validation Loss: 108.1555\n",
      "Epoch 23/100, Training Loss: 105.8930, Validation Loss: 104.5507\n",
      "Epoch 24/100, Training Loss: 96.8345, Validation Loss: 95.1442\n",
      "Epoch 25/100, Training Loss: 99.9345, Validation Loss: 94.5215\n",
      "Epoch 26/100, Training Loss: 91.0988, Validation Loss: 103.3358\n",
      "Epoch 27/100, Training Loss: 91.3252, Validation Loss: 106.9733\n",
      "Epoch 28/100, Training Loss: 95.5800, Validation Loss: 89.3479\n",
      "Epoch 29/100, Training Loss: 82.8736, Validation Loss: 90.2442\n",
      "Epoch 30/100, Training Loss: 84.2364, Validation Loss: 88.2582\n",
      "Epoch 31/100, Training Loss: 104.2576, Validation Loss: 91.8277\n",
      "Epoch 32/100, Training Loss: 98.5491, Validation Loss: 103.5452\n",
      "Epoch 33/100, Training Loss: 102.6396, Validation Loss: 85.7949\n",
      "Epoch 34/100, Training Loss: 107.6172, Validation Loss: 98.5496\n",
      "Epoch 35/100, Training Loss: 91.7614, Validation Loss: 100.9815\n",
      "Epoch 36/100, Training Loss: 92.1644, Validation Loss: 94.0210\n",
      "Epoch 37/100, Training Loss: 78.6422, Validation Loss: 90.5118\n",
      "Epoch 38/100, Training Loss: 83.5241, Validation Loss: 97.8670\n",
      "Epoch 39/100, Training Loss: 81.3260, Validation Loss: 96.9721\n",
      "Epoch 40/100, Training Loss: 80.5075, Validation Loss: 88.2372\n",
      "Epoch 41/100, Training Loss: 88.3989, Validation Loss: 96.9018\n",
      "Epoch 42/100, Training Loss: 86.3506, Validation Loss: 91.5106\n",
      "Epoch 43/100, Training Loss: 86.8694, Validation Loss: 97.3808\n",
      "Epoch 44/100, Training Loss: 92.5734, Validation Loss: 83.5042\n",
      "Epoch 45/100, Training Loss: 93.8924, Validation Loss: 85.1707\n",
      "Epoch 46/100, Training Loss: 82.5975, Validation Loss: 88.1216\n",
      "Epoch 47/100, Training Loss: 85.8832, Validation Loss: 112.1430\n",
      "Epoch 48/100, Training Loss: 89.2991, Validation Loss: 94.1236\n",
      "Epoch 49/100, Training Loss: 85.2318, Validation Loss: 99.3321\n",
      "Epoch 50/100, Training Loss: 79.7601, Validation Loss: 106.4689\n",
      "Epoch 51/100, Training Loss: 96.6897, Validation Loss: 152.6788\n",
      "Epoch 52/100, Training Loss: 101.5925, Validation Loss: 88.9476\n",
      "Epoch 53/100, Training Loss: 79.4301, Validation Loss: 83.4860\n",
      "Epoch 54/100, Training Loss: 92.3446, Validation Loss: 118.3554\n",
      "Epoch 55/100, Training Loss: 102.1750, Validation Loss: 115.5938\n",
      "Epoch 56/100, Training Loss: 81.8883, Validation Loss: 87.6379\n",
      "Epoch 57/100, Training Loss: 81.5269, Validation Loss: 104.4962\n",
      "Epoch 58/100, Training Loss: 109.1789, Validation Loss: 83.2727\n",
      "Epoch 59/100, Training Loss: 87.0477, Validation Loss: 80.6761\n",
      "Epoch 60/100, Training Loss: 75.5319, Validation Loss: 81.5760\n",
      "Epoch 61/100, Training Loss: 79.3903, Validation Loss: 80.6582\n",
      "Epoch 62/100, Training Loss: 87.9904, Validation Loss: 82.6465\n",
      "Epoch 63/100, Training Loss: 85.8727, Validation Loss: 87.9462\n",
      "Epoch 64/100, Training Loss: 83.0587, Validation Loss: 81.2799\n",
      "Epoch 65/100, Training Loss: 79.1407, Validation Loss: 110.0790\n",
      "Epoch 66/100, Training Loss: 84.0887, Validation Loss: 80.0576\n",
      "Epoch 67/100, Training Loss: 87.5964, Validation Loss: 85.2081\n",
      "Epoch 68/100, Training Loss: 85.9597, Validation Loss: 91.9336\n",
      "Epoch 69/100, Training Loss: 89.8347, Validation Loss: 94.4793\n",
      "Epoch 70/100, Training Loss: 89.7674, Validation Loss: 95.3979\n",
      "Epoch 71/100, Training Loss: 85.4261, Validation Loss: 93.4928\n",
      "Epoch 72/100, Training Loss: 87.0758, Validation Loss: 86.3870\n",
      "Epoch 73/100, Training Loss: 82.0630, Validation Loss: 90.4935\n",
      "Epoch 74/100, Training Loss: 82.8268, Validation Loss: 79.5562\n",
      "Epoch 75/100, Training Loss: 89.7432, Validation Loss: 107.1909\n",
      "Epoch 76/100, Training Loss: 105.2165, Validation Loss: 102.9299\n",
      "Epoch 77/100, Training Loss: 90.1358, Validation Loss: 94.3973\n",
      "Epoch 78/100, Training Loss: 90.9358, Validation Loss: 90.9399\n",
      "Epoch 79/100, Training Loss: 87.0926, Validation Loss: 79.5162\n",
      "Epoch 80/100, Training Loss: 80.9650, Validation Loss: 91.6726\n",
      "Epoch 81/100, Training Loss: 82.6361, Validation Loss: 101.2200\n",
      "Epoch 82/100, Training Loss: 82.2347, Validation Loss: 95.4408\n",
      "Epoch 83/100, Training Loss: 84.2332, Validation Loss: 83.5432\n",
      "Epoch 84/100, Training Loss: 98.8446, Validation Loss: 84.6054\n",
      "Epoch 85/100, Training Loss: 92.6708, Validation Loss: 100.6743\n",
      "Epoch 86/100, Training Loss: 93.0526, Validation Loss: 96.5406\n",
      "Epoch 87/100, Training Loss: 87.6482, Validation Loss: 77.9103\n",
      "Epoch 88/100, Training Loss: 95.4928, Validation Loss: 83.2758\n",
      "Epoch 89/100, Training Loss: 93.7858, Validation Loss: 79.6249\n",
      "Epoch 90/100, Training Loss: 84.4895, Validation Loss: 92.1027\n",
      "Epoch 91/100, Training Loss: 90.4273, Validation Loss: 104.5606\n",
      "Epoch 92/100, Training Loss: 88.5962, Validation Loss: 95.7367\n",
      "Epoch 93/100, Training Loss: 89.4459, Validation Loss: 95.5415\n",
      "Epoch 94/100, Training Loss: 92.6500, Validation Loss: 79.8241\n",
      "Epoch 95/100, Training Loss: 79.2270, Validation Loss: 86.6635\n",
      "Epoch 96/100, Training Loss: 81.1230, Validation Loss: 80.9964\n",
      "Epoch 97/100, Training Loss: 77.1198, Validation Loss: 82.5562\n",
      "Epoch 98/100, Training Loss: 75.0141, Validation Loss: 100.0226\n",
      "Epoch 99/100, Training Loss: 84.7153, Validation Loss: 86.0419\n",
      "Epoch 100/100, Training Loss: 81.7100, Validation Loss: 79.0444\n",
      "tensor([[ 923.8517,    8.0100],\n",
      "        [1069.6403,   11.5796],\n",
      "        [1022.0869,   10.2951],\n",
      "        [ 890.7329,    7.3100],\n",
      "        [1004.3851,   10.0661],\n",
      "        [1011.2941,   10.2496],\n",
      "        [ 881.2271,    7.1387],\n",
      "        [ 910.8077,    7.7946]], device='cuda:0')\n",
      "tensor([[ 911.0000,    6.1000],\n",
      "        [1066.0000,    3.2000],\n",
      "        [1005.0000,    9.4000],\n",
      "        [ 883.0000,    5.8000],\n",
      "        [1006.0000,   10.0000],\n",
      "        [1017.0000,   11.3000],\n",
      "        [ 879.0000,    6.8000],\n",
      "        [ 924.0000,    6.1000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Initialize variable to track loss and accuracies for each epochs\n",
    "y_loss = {} \n",
    "y_loss['train'] = []\n",
    "y_loss['val'] = []\n",
    "y_err = {}\n",
    "y_err['train'] = []\n",
    "y_err['val'] = []\n",
    "x_epoch = []\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "dataset_sizes = {'train': len(train_loader), 'val': len(val_loader)}\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs.to(device))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets.to(device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            val_outputs = model(val_inputs.to(device))\n",
    "            val_loss += criterion((val_outputs/4096)*, val_targets.to(device)).item()\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss:.4f}, Validation Loss: {average_val_loss:.4f}')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_inputs, val_targets in val_loader:\n",
    "        val_outputs = model(val_inputs.to(device))\n",
    "        print(val_outputs)\n",
    "        print(val_targets.to(device))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625daf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbb28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90c605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed327b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0400d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cdd92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233ca52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e2a91316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # start\n",
    "# start = time.time()\n",
    "# for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "#     for phase in ['train', 'val']:        \n",
    "#         running_loss = 0.0\n",
    "#         running_corrects = 0.0\n",
    "#         count = 0\n",
    "#         total = 0\n",
    "#         if(phase == 'train'):\n",
    "#             for images, labels in train_loader:\n",
    "#                 images = images.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "        \n",
    "#                 # Forward pass\n",
    "#                 outputs = model(images)\n",
    "#                 _, preds = torch.max(outputs.data, 1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "#                 # Backward and optimize\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 # the class with the highest energy is what we choose as prediction        \n",
    "#                 running_loss += loss.item()              \n",
    "#                 total += labels.size(0)\n",
    "#                 running_corrects += (preds == labels).sum().item()\n",
    "        \n",
    "#             epoch_loss = running_loss / total\n",
    "#             epoch_acc = running_corrects / total\n",
    "# #             if (i+1) % 20 == 0:\n",
    "# #             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "#             y_loss[phase].append(epoch_loss)\n",
    "#             y_err[phase].append(1.0 - epoch_acc)\n",
    "# #             scheduler.step()\n",
    "\n",
    "            \n",
    "#         elif(phase == 'val'):\n",
    "#             for i, (images, labels) in enumerate(val_loader):\n",
    "#                 images = images.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 outputs = model(images)\n",
    "#                 _, preds = torch.max(outputs.data, 1)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 # the class with the highest energy is what we choose as prediction\n",
    "#                 running_loss += loss.item()              \n",
    "#                 total += labels.size(0)\n",
    "#                 running_corrects += (preds == labels).sum().item()               \n",
    "                \n",
    "#             epoch_loss = running_loss / total\n",
    "#             epoch_acc = running_corrects / total\n",
    "# #             if (i+1) % 200 == 0:\n",
    "# #             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "#             y_loss[phase].append(epoch_loss)\n",
    "#             y_err[phase].append(1.0 - epoch_acc)\n",
    "\n",
    "#             x_epoch.append(epoch)  \n",
    "            \n",
    "# print(f'training time: {(time.time()-start)} sec')\n",
    "# # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c31a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022008c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed5b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9caef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf5c0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
